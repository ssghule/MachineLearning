import sklearn as sk;
import numpy as np;
import pandas as pd;
from sklearn.model_selection import KFold;
from sklearn import preprocessing;

from sklearn import tree;
from sklearn.naive_bayes import GaussianNB;
from sklearn import linear_model, datasets;
from sklearn import neighbors;
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm


def load_data(dloc):
    # List of data points
    data_list = []

    # Column labels
    col_labels = ["age", "workclass", "fnlwgt", "education", "education_num", "marital_status", "occupation",
                  "relationship", "race", "sex", "capital_gain", "capital_loss", "hours_per_week", "native_country",
                  "income_class"]
    # Open the data file
    with open(dloc, "r+") as f:
        # Process each line in the data file
        for line in f:
            # Clean the CSV data line
            line = line.replace(" ", "");
            line = line.replace(".", "");
            line = line.replace("\"", "");
            line = line.strip().split(",");
            # Add the list containing the datapoint on this in the CSV to the
            # list containing all data points
            data_list.append(line);
            # Creating a dataframe with the data
    df = pd.DataFrame(data=data_list, columns=col_labels);
    return df;


def preprocess_data(df):
    df = df[df.age != ""]  # removing empty rows
    df = df[df.workclass != "?"]  # removing lines with "?" in place of missing value
    df = df[df.occupation != "?"]
    df = df[df.native_country != "?"]
    df['age'] = df['age'].astype(int);
    df['workclass'] = df['workclass'].astype('str');
    df['fnlwgt'] = df['fnlwgt'].astype(int);
    df['education'] = df['education'].astype('str');
    df['education_num'] = df['education_num'].astype(int);
    df['marital_status'] = df['marital_status'].astype('str');
    df['occupation'] = df['occupation'].astype('str');
    df['relationship'] = df['relationship'].astype('str');
    df['race'] = df['race'].astype('str');
    df['sex'] = df['sex'].astype('str');
    df['capital_gain'] = df['capital_gain'].astype(int);
    df['capital_loss'] = df['capital_loss'].astype(int);
    df['hours_per_week'] = df['hours_per_week'].astype(int);
    df['native_country'] = df['native_country'].astype('str');
    df['income_class'] = df['income_class'].astype('str');
    df = df.reset_index();  # resets the index because some records have been deleted

    # splitting "age" attribute into bins and creating a numeric label for each bin
    age_bins = [0, 20, 30, 40, 50, 60, 70, 80, 100]
    # age_cat_names = ['<20', '21-30',  '31-40', '41-50', '51-60', '61-70', '71-80', '>81']
    age_cat_names = [1, 2, 3, 4, 5, 6, 7, 8]
    age_cat = pd.cut(df['age'], age_bins, labels=age_cat_names)
    df['age_label'] = pd.cut(df['age'], age_bins, labels=age_cat_names)
    df['age_label'] = df['age_label'].astype(int);

    # Creating a label column for each of the categorical variables
    le = preprocessing.LabelEncoder()
    le.fit(df['workclass']);
    df['workclass_label'] = le.transform(df['workclass']);

    le.fit(df['marital_status']);
    df['marital_status_label'] = le.transform(df['marital_status']);

    le.fit(df['occupation']);
    df['occupation_label'] = le.transform(df['occupation']);

    le.fit(df['relationship']);
    df['relationship_label'] = le.transform(df['relationship']);

    le.fit(df['race']);
    df['race_label'] = le.transform(df['race']);

    le.fit(df['sex']);
    df['sex_label'] = le.transform(df['sex']);

    le.fit(df['native_country']);
    df['native_country_label'] = le.transform(df['native_country']);

    return df;

if __name__ == "__main__":
    # The data directory
    data_loc = "F:\\Mildred\\AML\\Programming Assignments\\Adult Data\\adult.data";
    data_loc1 = "F:\\Mildred\\AML\\Programming Assignments\\Adult Data\\adult.test";
    # Load the data into memory
    df = load_data(data_loc);
    df1 = load_data(data_loc1);
    train = preprocess_data(df);
    test = preprocess_data(df1);
    # attributes to be selected for training
    col_labels = ["age", "fnlwgt", "workclass_label", "education_num", "marital_status_label", "occupation_label",
                  "relationship_label", "race_label", "sex_label", "capital_gain", "capital_loss", "hours_per_week",
                  "native_country_label"]
    trainX = train[col_labels];
    # target variable
    trainY = train["income_class"];
    testX = test[col_labels];
    testY = test["income_class"];

def learn_randomForest(X, Y):
    noOfTrees = [5, 6, 7, 8, 9, 10, 11]
    best_model = [None, 0, float("-inf"), 0]
    depth = [10, 12, 14, 16, 18, 20, 22, 24]
    # Create the object that will split the training set into training and
    # validation sets
    kf = KFold(n_splits=10);
    col_labels = ["age", "fnlwgt", "workclass_label", "education_num", "marital_status_label", "occupation_label",
                  "relationship_label", "race_label", "sex_label", "capital_gain", "capital_loss", "hours_per_week",
                  "native_country_label"]
    for(train, test), number, d in zip(kf.split(X), noOfTrees, depth):
        # Pull out the records and labels that will be used to train this model
        train_X = X.ix[train, col_labels]
        train_Y = Y.ix[train]
        valid_X = X.ix[test, col_labels]
        valid_Y = Y.ix[test]

        clf = RandomForestClassifier(n_estimators = number)
        clf = clf.fit(train_X, train_Y)
        accuracy = clf.score(valid_X, valid_Y)

        if(accuracy > best_model[2]):
            best_model = [clf, number, accuracy, d]
    return best_model

best_randomForest = learn_randomForest(trainX, trainY)
print(best_randomForest)
randomForestAccuracy = best_randomForest[0].score(testX, testY)
print(randomForestAccuracy)

def learn_adaboost(X, Y):
    estimator = [50, 60, 70, 80, 90, 100, 110]
    #depth = []
    best_model = [None, 0, float("-inf")]
    # Create the object that will split the training set into training and
    # validation sets
    kf = KFold(n_splits=10);
    col_labels = ["age", "fnlwgt", "workclass_label", "education_num", "marital_status_label", "occupation_label",
                  "relationship_label", "race_label", "sex_label", "capital_gain", "capital_loss", "hours_per_week",
                  "native_country_label"]
    for(train, test), number in zip(kf.split(X), estimator):
        # Pull out the records and labels that will be used to train this model
        train_X = X.ix[train, col_labels]
        train_Y = Y.ix[train]
        valid_X = X.ix[test, col_labels]
        valid_Y = Y.ix[test]

        clf = AdaBoostClassifier(n_estimators = number)
        clf = clf.fit(train_X, train_Y)
        accuracy = clf.score(valid_X, valid_Y)

        if(accuracy > best_model[2]):
            best_model = [clf, number, accuracy]
    return best_model

best_adaboost = learn_adaboost(trainX, trainY)
print(best_adaboost)
adaboostAccuracy = best_adaboost[0].score(testX, testY)
print(adaboostAccuracy)

def learn_SVM(X, Y):
    #an array of different k values to be tried on the training set
    k_values = [0.25,1,2,3,4,5];
    best_model = [ None, 0, float("-inf") ];
    # Create the object that will split the training set into training and
    # validation sets
    kf = KFold(n_splits=5);
    col_labels= ["age","fnlwgt","workclass_label","education_num","marital_status_label","occupation_label","relationship_label","race_label","sex_label","capital_gain","capital_loss","hours_per_week","native_country_label"]    #
    # Iterate over each of the 10 splits on the data set
    for (train, test), k in zip(kf.split(X),k_values):
        # Pull out the records and labels that will be used to train this model     
        train_X=X.ix[train,col_labels];
        train_Y=Y.ix[train];
        valid_X =X.ix[test,col_labels];
        valid_Y =Y.ix[test];
        # Create the classifier object
        clf = svm.SVC(C=k);
        # Learn the model on the training data that will be used for this
        # fold
        clf = clf.fit(train_X, train_Y);
        # Evaluate the learned model on the validation set
        accuracy = clf.score(valid_X, valid_Y);
        # Check whether or not this learned model is the most accuracy model
        if accuracy > best_model[2]:
            # Update best_model so that it holds this learned model and its
            # associated accuracy and hyper-parameter information
            best_model = [ clf, k, accuracy ];
    return best_model;

best_SVM = learn_SVM(trainX, trainY);
print(best_SVM);
SVM_accuracy = best_SVM[0].score(testX, testY);
print(SVM_accuracy);
